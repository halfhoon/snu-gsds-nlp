{"cells":[{"cell_type":"markdown","metadata":{"id":"lhs1disrZWXy"},"source":["# 2. Sentiment Analysis using Neural Network\n","\n","In class, we learned sequence representation and how language models are developed for different tasks. In this assignment, we will implement two neural network models for sentiment analysis task using IMDB dataset. Sentiment analysis in Natural Language Processing (NLP) is a task that involves classifying sentences or text into different categories based on the sentiment expressed. It aims to determine whether the sentiment of the text is positive, or negative. This analysis helps in understanding the overall opinion or emotion conveyed by the text.\n","\n","Please note that this assignment is built and tested under *Google Colaboratory*. If you work on a local machine, you need to handle version issue on your own. Please complete the given jupyter notebook file and submit it along with your answer to this latex file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGurOjzMY1vv"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"M-5EqJZlYseM"},"source":["## Data Preprocessing\n","\n","In this assignment, we will use TorchText package to deal with the data. There are a few ways to process the data, but this package makes this procedure much easier. Now we will go over the next steps with TorchText:\n","\n","- Preprocessing\n","- Split into train and test set\n","- Build dataset\n","- Building vocabulary\n","- Batching the data\n","\n","Once you load the data, there will be two columns, review comment and sentiment label (1 for positive and 0 for negative).\n","Out of 50000 sample data, we will use 30000 as our training set and rest of the data as our test set.\n","You can refer to [here](https://torchtext.readthedocs.io/en/latest/index.html) for more information on the TorchText."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIG5GguwiXZK"},"outputs":[],"source":["!pip install torchtext==0.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pil9y5s0PcOF"},"outputs":[],"source":["import urllib.request\n","import pandas as pd\n","from torchtext import data, datasets\n","from torchtext.vocab import Vocab\n","from torchtext.data import TabularDataset\n","from torchtext.data import Iterator\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBIS93RDWCWh"},"outputs":[],"source":["# load data\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vlt0g24XWDhG"},"outputs":[],"source":["df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwrEtY3YWGMS"},"outputs":[],"source":["print('Total number of sample data: {}'.format(len(df)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlWMZKeHWJls"},"outputs":[],"source":["train_df = df[:30000]\n","test_df = df[30000:]\n","\n","train_df.to_csv(\"train_data.csv\", index=False)\n","test_df.to_csv(\"test_data.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YW4JkUJtWKuW"},"outputs":[],"source":["# data preprocessing\n","TEXT = data.Field(sequential=True,\n","                  use_vocab=True,\n","                  tokenize=str.split,\n","                  lower=True,\n","                  batch_first=True,\n","                  fix_length=100)\n","\n","LABEL = data.Field(sequential=False,\n","                   use_vocab=False,\n","                   batch_first=False,\n","                   is_target=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qE-s8tWdWRmH"},"outputs":[],"source":["# build dataset\n","train_data, test_data = TabularDataset.splits(\n","        path='.', train='train_data.csv', test='test_data.csv', format='csv',\n","        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aT1Brmh8WTUV"},"outputs":[],"source":["print('number of training data : {}'.format(len(train_data)))\n","print('number of test data : {}'.format(len(test_data)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tnwO8cnEWiDP"},"outputs":[],"source":["print(train_data.fields.items())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDllSsureQ6v"},"outputs":[],"source":["print(train_data[0].text)\n","print(train_data[0].label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCwqPZfzeYl7"},"outputs":[],"source":["print(vars(train_data[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Wy90HN9Wj67"},"outputs":[],"source":["# build vocabulary\n","TEXT.build_vocab(train_data, min_freq=10, max_size=10000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsHXDKTEWvPO"},"outputs":[],"source":["print('size of the vocabulary : {}'.format(len(TEXT.vocab))) # includes <unk> and <pad>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fn0hJcSbWx6W"},"outputs":[],"source":["print(TEXT.vocab.stoi)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9iXxEd7XS8Z"},"outputs":[],"source":["# build data loader and batching the data\n","batch_size = 64\n","train_loader = Iterator(dataset=train_data, batch_size = batch_size)\n","test_loader = Iterator(dataset=test_data, batch_size = batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYZiR0YvXVgf"},"outputs":[],"source":["print('number of mini-batches for training data : {}'.format(len(train_loader)))\n","print('number of mini-batches for test data : {}'.format(len(test_loader)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bet56QmhXXYD"},"outputs":[],"source":["batch = next(iter(train_loader))\n","print(type(batch))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqkMXibKXaq3"},"outputs":[],"source":["print(batch.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsNt6WuDXhHE"},"outputs":[],"source":["print(batch.label)"]},{"cell_type":"markdown","metadata":{"id":"odFUQcT1izne"},"source":["## Problem 2-1. Multilayer Perceptron (MLP) (30 pts)\n","\n","In this question, we are going to implement a simple Multilayer Perceptron (MLP) model to classify IMDB dataset. MLP is the classical type of neural network, and they are comprised of one or more layers of neurons. Data is fed to the input layer, there may be one or more hidden layers providing levels of abstraction, and predictions will be made on the output layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zr1l9eXFmlv-"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"03HnknkuvFrE"},"source":["### Problem 2-1 (a) (10 pts)\n","\n","Implement a two-layer fully-connected neural network. Your model should contain an embedding layer to represent a word in a dense vector representation. Use ReLU for activation function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2MgCMmKji1w"},"outputs":[],"source":["class Linear(nn.Module):\n","    def __init__(self, input_size, hidden_size, embed_dim, num_classes):\n","        super(Linear, self).__init__()\n","        #################### YOUR CODE (3 lines) #####################\n","\n","\n","        ###############################################################\n","\n","    def forward(self, text):\n","        #################### YOUR CODE (3-5 lines) #####################\n","        # hint: you can use F.relu and F.softmax\n","\n","\n","\n","        ###############################################################\n","        return preds"]},{"cell_type":"markdown","metadata":{"id":"rh1PiRmGxTtB"},"source":["### Problem 2-1 (b) (10 pts)\n","\n","Implement a function to check the accuracy of your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssC7GxiJyPiw"},"outputs":[],"source":["def model_accuracy(output, target):\n","    #################### YOUR CODE (2-3 lines) #####################\n","\n","\n","\n","    ###############################################################\n","    return accuracy"]},{"cell_type":"markdown","metadata":{"id":"2aLakQnpyXS2"},"source":["### Problem 2-1 (c) (5 pts)\n","\n","Complete the code below to train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGaA84zSlYFb"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion):\n","\n","    loss_ep = 0\n","    acc_ep = 0\n","\n","    for batch in iterator:\n","        optimizer.zero_grad()\n","\n","        #################### YOUR CODE (1-2 lines) #####################\n","\n","\n","        ###############################################################\n","        loss = criterion(output, batch.label.squeeze())\n","        acc = model_accuracy(output, batch.label)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_ep += loss.item()\n","        acc_ep += acc.item()\n","\n","    return loss_ep / len(iterator), acc_ep / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"VyyXGom71KMT"},"source":["### Problem 2-1 (d) (5 pts)\n","\n","Complete the code below to evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kw2YtEgG0gXX"},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","\n","    loss_ep = 0\n","    acc_ep = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in iterator:\n","            #################### YOUR CODE (1-2 lines) #####################\n","\n","\n","            ###############################################################\n","            loss = criterion(output, batch.label)\n","            acc = model_accuracy(output, batch.label)\n","\n","            loss_ep += loss.item()\n","            acc_ep += acc.item()\n","\n","    return loss_ep / len(iterator), acc_ep / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"vL_aVpXd2ZVI"},"source":["Now let's see the model performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8QjG2I8xRgh"},"outputs":[],"source":["def train_the_model(epochs, model, train_loader, valid_loader, optimizer, criterion):\n","\n","    for epoch in range(epochs):\n","\n","        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n","        valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n","\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n","\n","    torch.save(model.state_dict(), 'saved_weights_linear.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsTmY9Wxulu7"},"outputs":[],"source":["# hyperparameter\n","num_epochs = 10\n","lr = 1e-4\n","max_token_length = 100\n","hidden_size = 100\n","embed_dim = 100\n","seed = 1\n","num_classes = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfLhdkTmmehD"},"outputs":[],"source":["loss_function = nn.CrossEntropyLoss()\n","linear_model = Linear(max_token_length, hidden_size, embed_dim, num_classes)\n","optimizer = torch.optim.Adam(linear_model.parameters(), lr=lr)\n","\n","train_the_model(num_epochs, linear_model, train_loader, test_loader, optimizer, loss_function)\n"]},{"cell_type":"markdown","metadata":{"id":"ZccIlM7P4o4O"},"source":["## Problem 2-2. Convolutional Neural Network (CNN) (20 pts)\n","\n","Next, we will perform sentimental analysis on the same dataset with Convolutional Neural Network (CNN). In a CNN, text is organised into a matrix, with each row representing a word embedding. The CNN’s convolutional layer scans the text like it would an image, breaks it down into features, and judges whether each feature matches the relevant label or not.\n","You can refer to [here](https://emnlp2014.org/papers/pdf/EMNLP2014181.pdf) for the use of CNN on text classification.\n"]},{"cell_type":"markdown","metadata":{"id":"3dZiilj0ZsyU"},"source":["### Problem 2-2 (a) (15 pts)\n","\n","Complete the code below for CNN. Again, your model will require an embedding layer to represent into dense vector. After getting embeddings, we will feed the tensors through the convolutional layer, applying the ReLu activation function following the convlutional layers. Then the tensors will be passed through pooling layers. Lastly, apply dropout to the concatenated filter outputs and subsequently pass them through a linear layer to *generate* our predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDDXPBzGLjLX"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, input_dim, embed_dim, n_filters, filter_sizes, num_classes, dropout_rate):\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        # convolutions with different size of filters\n","        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1,\n","                                              out_channels=n_filters,\n","                                              kernel_size=(filter_size, embed_dim))\n","                                    for filter_size in filter_sizes])\n","\n","        self.fc = nn.Linear(len(filter_sizes)*n_filters, num_classes)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","\n","    def forward(self, text):\n","        # text = [batch_size, sen_len]\n","\n","        embedded = self.embedding(text)\n","        # embedded = [batch_size, sen_len, embed_dim]\n","\n","        embedded = embedded.unsqueeze(1)\n","        # embedded = [batch_size, 1, sen_len, embed_dim]\n","\n","        #################### YOUR CODE (4 lines) #####################\n","        # hint: you can use F.relu, F.max_pool1d, and torch.cat\n","\n","        # After getting embeddings, we will feed the tensors through the convolutional layer, applying the ReLu activation function following the convlutional layers.\n","        # output size of the n-convolutional layer = [batch_size, n_filters, sen_len - filter_sizes[n] + 1]\n","\n","        # Then the tensors will be passed through pooling layers.\n","        # output size of the n-pooling layer = [batch_size, n_filters]\n","\n","        # Lastly, apply dropout to the concatenated filter outputs and subsequently pass them through a linear layer to generate our predictions.\n","        # output size of the dropout = [batch size, n_filters * len(filter_sizes)]\n","\n","\n","\n","\n","\n","        ###############################################################\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"W6YgprZeeyi_"},"source":["Now let's see the model performance.\n","\n","We will use the same function that we used in MLP model for model training and evaluation.\n","It will take about 30 minutes to run 5 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3fmVYj1LjFj"},"outputs":[],"source":["# hyperparameter\n","num_epochs = 5\n","lr = 1e-4\n","input_dim = len(TEXT.vocab)\n","embed_dim = 300\n","num_filters = 100\n","filter_sizes = [3,5,7]\n","dropout_rate = 0.25\n","num_classes = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0WujCcBwLjBo"},"outputs":[],"source":["cnn_model = CNN(input_dim, embed_dim, num_filters, filter_sizes, num_classes, dropout_rate)\n","\n","optimizer = torch.optim.Adam(cnn_model.parameters(), lr=lr)\n","train_the_model(num_epochs, cnn_model, train_loader, test_loader, optimizer, loss_function)\n"]},{"cell_type":"markdown","metadata":{"id":"wJJaN8CTwnzK"},"source":["### Problem 2-2 (b) (5 pts)\n","\n","Compare the linear model (problem 1) and CNN (problem 2), and discuss their advantages and limitations."]},{"cell_type":"markdown","metadata":{"id":"WYXPjGJtwsIT"},"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHvpojwo8xYzhNI84yloKW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}